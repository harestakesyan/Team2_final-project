# Team 2 Final Project - Categorical variables EDA - Zhe  
## Feature selection  
In the EDA, t-tests and Chi-square tests were used to compare whether the response variables `age`, `gender`, `height`, `weight`, `ap_hi`, `ap_lo`, `cholesterol`, `gluc`, `smoke`, `alco`, `active` and `bmi` have effects on the predictor variable `cardio`, and the p-values of the tests were shown in the table below:  
```{r results='markup'}
tab <- matrix(c( '<2e-16','0.6', '8e-07', '<2e-16', '<2e-16', '<2e-16', '<2e-16', '<2e-16', '1e-06', '0.006', '<2e-16', '<2e-16'), ncol=12, byrow = TRUE)
rownames(tab) <- "p_value"
colnames(tab) <- c("Age_year", "Gender", "Height","Weight","Systolic(ap_hi)", "Diastolic(ap_low)", "Cholesterol","Glucose","smoke", "Alcohol", "Physical_activity","BMI")
tab <- as.table(tab)
xkabledply(tab, "p-value Comparison")  
```

**From the above table, we found that all the response variables have relationship with the predictor variable `cardio` except the `gender` variable. Next we further confirmed this and tested the strength of the relationships with the `bestglm` function.  

```{r bestglm, results='markup'}
loadPkg("bestglm")
loadPkg("leaps")
loadPkg("ggplot2")
cardio_clean_final_glm <- cardio_clean_final
colnames(cardio_clean_final_glm)[12] <- "y"
cardio_clean_final_glm <- cardio_clean_final_glm[c("age", "gender", "height", "weight","ap_hi", "ap_lo", "cholesterol", "gluc", "smoke", "alco", "active", "bmi", "y")]

res.bestglm <- bestglm(Xy = cardio_clean_final_glm, family = binomial,
            IC = "AIC", method = "exhaustive")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm") 
unloadPkg("leaps") 
```
`gender` and `bmi` were excluded from the model that has lowest criterion value, indicating these two valuables may not have strong effects on `cardio`. But among the 5 models the function generated, `gender` was voted as `FALSE` three time, while `bmi` was only voted twice, so we are not confidence in removing these two valuables from the following modeling steps. In the KNN modeling, we compare whether these two variables can affect the model performances.   

## **K-Nearest Neighbor (KNN)**   
### **Convert factors into numeric, and Scale/center the data**   
```{r results='markup'}
<<<<<<< HEAD
cardio_vs_cholesterol = table(cardio_clean_final$cardio, cardio_clean_final$cholesterol)
chi_cholesterol = chisq.test(cardio_vs_cholesterol)
chi_cholesterol
```  
The p-value from the Chi-square test is less than 2e-16, which is smaller than 0.05. So we can reject the null hypothesis. This result indicating `cholesterol` level will contribute to the onset of cardiovascular diseases.  

**3. Whether `cardio` and `gluc` are independent.**  
- H0: cardio and gluc are independent.  
- H1: they are not independent.  
```{r results='markup'}
cardio_vs_glucose = table(cardio_clean_final$cardio, cardio_clean_final$gluc)
chi_glucose = chisq.test(cardio_vs_glucose)
chi_glucose
```  
The p-value from the Chi-square test is less than 2e-16, which is smaller than 0.05. So we can reject the null hypothesis. This result indicating glucose (`gluc`) level will contribute to the onset of cardiovascular diseases.  

**4. Whether `cardio` and `smoke` are independent.**  
- H0: cardio and smoke are independent.  
- H1: they are not independent.  
```{r results='markup'}
cardio_vs_smoke = table(cardio_clean_final$cardio, cardio_clean_final$smoke)
chi_smoke = chisq.test(cardio_vs_smoke)
chi_smoke
```  
The p-value from the Chi-square test is 3e-08, which is smaller than 0.05. So we can reject the null hypothesis. This result indicating smoke or not will contribute to the onset of cardiovascular diseases.  

**5. Whether `cardio` and `alco` are independent.**  
- H0: cardio and alco are independent.  
- H1: they are not independent.  
```{r results='markup'}
cardio_vs_alco = table(cardio_clean_final$cardio, cardio_clean_final$alco)
chi_alco = chisq.test(cardio_vs_alco)
chi_alco
```  
The p-value from the Chi-square test is 0.002, which is smaller than 0.05. So we can reject the null hypothesis. This result indicating whether the patient consumes alcohol or not will contribute to the onset of cardiovascular diseases.  

**6. Whether `cardio` and `active` are independent.**  
- H0: cardio and active are independent.  
- H1: they are not independent.  
```{r results='markup'}
cardio_vs_active = table(cardio_clean_final$cardio, cardio_clean_final$active)
chi_active = chisq.test(cardio_vs_active)
chi_active
```  
The p-value from the Chi-square test is less then 2e-16, which is smaller than 0.05. So we can reject the null hypothesis. This result indicating whether the patient is physically active or not will contribute to the onset of cardiovascular diseases.  

**Based on the Chi-square tests we performed, we found `gender` is not contribute to the presence or absence of cardiovascular disease, while `cholesterol`, `gluc`, `smoke`, `alco` and `active` are the variables that can affect whether the patient has cardiovascular disease or not.**

# Feature selection
**From the t-tests and Chi-square tests, we found that all the response variables have relationship with the predictor variable `cardio` except the `gender` variable. Next we tested the strength of the relationships with the `bestglm` function.

```{r bestglm, results='markup'}
loadPkg("bestglm")
loadPkg("leaps")
loadPkg("ggplot2")
cardio_clean_final_glm <- cardio_clean_final
colnames(cardio_clean_final_glm)[12] <- "y"
cardio_clean_final_glm <- cardio_clean_final_glm[c("age", "gender", "height", "weight","ap_hi", "ap_lo", "cholesterol", "gluc", "smoke", "alco", "active", "bmi", "y")]

res.bestglm <- bestglm(Xy = cardio_clean_final_glm, family = binomial,
            IC = "AIC", method = "exhaustive")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm") 
unloadPkg("leaps") 
```
Based on the results, `gender` and `bmi` may not have strong effects on `cardio`. But we still want to compare whether these two variables will affect the KNN modeling. 

# **K-Nearest Neighbor (KNN)**   
## **Convert facrors into numeric, and scale and center the data**   
```{r results='markup'}
=======
>>>>>>> main
cardio_num <- cardio_clean_final_glm
colnames(cardio_num)[13] <- "cardioi"
cardio_num[1:12] <- sapply(cardio_num[1:12],as.numeric)
cardio_scale <- cardio_num
cardio_scale[1:12] <- as.data.frame(scale(cardio_num[1:12], center = TRUE, scale = TRUE))
str(cardio_scale)
```  
<<<<<<< HEAD
## **KNN modeling using variables with all 13 variables.**  
## Train / Test split:  
The dataframe was seperated into training subset (70% of the total dataframe) and test subset (30% of the total dataframe)
=======
Dataframe structure after pre-modeling treatment was shown in the above table. All the response variables were converted in to nueric, and were scaled and centered. They are ready for KNN modeling.  

### **KNN modeling using variables with all 13 variables.**  
#### Train / Test split:  

The dataframe was split into training subset (70% of the total dataframe) and test subset (30% of the total dataframe).  
>>>>>>> main
```{r}
set.seed(42)
cardio_sample <- sample(2, nrow(cardio_scale), replace=TRUE, prob=c(0.7, 0.3))
cardio_training_x <- cardio_scale[cardio_sample==1, 1:12]
cardio_test_x <- cardio_scale[cardio_sample==2, 1:12]
cardio_training_y <- cardio_scale[cardio_sample==1, 13]
cardio_test_y <- cardio_scale[cardio_sample==2, 13]
```

#### Looking for the best K value:  
Odd numbers from 3 to 40 were test one by one to get the best K value. The accuracy values from the model that used these K values were shown in the table and the line graph. According the results, K = 19 is the best K value if we use all the 13 variables for modeling.  

```{r}
loadPkg("FNN")
loadPkg("gmodels")
loadPkg("caret")
K_test <- seq(3, 40, by = 2)
ResultDf = data.frame()
for (k_v in K_test) {
  cardio_pred <- knn(train = cardio_training_x, test = cardio_test_x, cl=cardio_training_y, k=k_v)
  cardio_PRED_Cross <- CrossTable(cardio_test_y, cardio_pred, prop.chisq = FALSE)
  
  cm = confusionMatrix(cardio_pred, reference = cardio_test_y )
  
  cmaccu = cm$overall['Accuracy']

  cmt = data.frame(k=k_v, Total.Accuracy = cmaccu, row.names = NULL ) 
  
  ResultDf = rbind(ResultDf, cmt)
}
```
```{r results='markup'}
xkabledply(ResultDf, "Total Accuracy Summary")
```

```{r}
library(ggplot2)
ggplot(ResultDf, aes(x=k, y=Total.Accuracy)) +
  geom_line(color = "skyblue", size = 1.5) +
  geom_point(size = 3, color = "blue") + 
  labs(title = "Total.Accuracy vs K")
```  
#### Modeling results with the best K value  

Detailed results and the confusion matrix using K=19 were shown below. The following information can be calculated from the confusion matrix:  
* Accuracy = 0.717  
* Precision = 0.732  
* Recall = 0.676  
* Specification = 0.757  
* F1 Score = 0.703  
The overall performance is acceptable, but we still want to see whether remove `gender` and `bmi` can improve the modeling.  

```{r results='markup'}
<<<<<<< HEAD
  cardio_pred_15 <- knn(train = cardio_training_x, test = cardio_test_x, cl=cardio_training_y, k=19)
  cardio_PRED_Cross <- CrossTable(cardio_test_y, cardio_pred, prop.chisq = FALSE)
  cm = confusionMatrix(cardio_pred_15, reference = cardio_test_y)
  cmaccu = cm$overall['Accuracy']
  print( paste("Accuracy_K_19 = ", cmaccu ) )
```
## **KNN modeling using variables without `gender` and `bmi`.**  
## Train / Test split:  
The dataframe was split into training subset (70% of the total dataframe) and test subset (30% of the total dataframe)
=======
  cardio_pred_19 <- knn(train = cardio_training_x, test = cardio_test_x, cl=cardio_training_y, k=19)
  cardio_PRED_Cross <- CrossTable(cardio_test_y, cardio_pred_19, prop.chisq = FALSE)
  cm = confusionMatrix(cardio_pred_19, reference = cardio_test_y)
  cmaccu = cm$overall['Accuracy']
  print( paste("Accuracy_K_19 = ", cmaccu ) )
```
### **KNN modeling using variables without `gender` and `bmi`.**  
#### Train / Test split:  
The dataframe was split into training subset (70% of the total dataframe) and test subset (30% of the total dataframe). The valuables `gender` and `bmi` were excluded from the train/test split.
>>>>>>> main
```{r}
cardio_scale2 <- cardio_scale[c(1, 3:11, 13)]
cardio_scale2
set.seed(42)
cardio_sample2 <- sample(2, nrow(cardio_scale2), replace=TRUE, prob=c(0.7, 0.3))
cardio_training_x2 <- cardio_scale2[cardio_sample2==1, 1:10]
cardio_test_x2 <- cardio_scale2[cardio_sample2==2, 1:10]
cardio_training_y2 <- cardio_scale2[cardio_sample2==1, 11]
cardio_test_y2 <- cardio_scale2[cardio_sample2==2, 11]
```

#### Looking for the best K value:  
Odd numbers from 3 to 40 were used for the test. The accuracy values from the model that used these K values were shown in the table and the line graph. According the results, K = 29 is the best K value for the KNN modeling without `gender` and `bmi`.

```{r}
K_test <- seq(3, 40, by = 2)
ResultDf2 = data.frame()
for (k_v in K_test) {
  cardio_pred2 <- knn(train = cardio_training_x2, test = cardio_test_x2, cl=cardio_training_y2, k=k_v)
  cardio_PRED_Cross2 <- CrossTable(cardio_test_y2, cardio_pred2, prop.chisq = FALSE)
  
  cm2 = confusionMatrix(cardio_pred2, reference = cardio_test_y2 )
  
  cmaccu2 = cm2$overall['Accuracy']

  cmt2 = data.frame(k=k_v, Total.Accuracy = cmaccu2, row.names = NULL ) 
  
  ResultDf2 = rbind(ResultDf2, cmt2)
}
```
```{r results='markup'}
xkabledply(ResultDf2, "Total Accuracy Summary")
```

```{r}
library(ggplot2)
ggplot(ResultDf2, aes(x=k, y=Total.Accuracy)) +
  geom_line(color = "skyblue", size = 1.5) +
  geom_point(size = 3, color = "blue") + 
  labs(title = "Total.Accuracy vs K")
```  

#### Modeling results with the best K value
Detailed results and the confusion matrix using K=29 were shown below. The following information can be calculated from the confusion matrix:  
* Accuracy = 0.722  
* Precision = 0.740  
* Recall = 0.676  
* Specification = 0.766  
* F1 Score = 0.753  

```{r results='markup'}
<<<<<<< HEAD
  cardio_pred_43 <- knn(train = cardio_training_x2, test = cardio_test_x2, cl=cardio_training_y2, k=29)
  cardio_PRED_Cross <- CrossTable(cardio_test_y2, cardio_pred2, prop.chisq = FALSE)
  cm2 = confusionMatrix(cardio_pred2, reference = cardio_test_y2 )
=======
  cardio_pred_29 <- knn(train = cardio_training_x2, test = cardio_test_x2, cl=cardio_training_y2, k=29)
  cardio_PRED_Cross <- CrossTable(cardio_test_y2, cardio_pred_29, prop.chisq = FALSE)
  cm2 = confusionMatrix(cardio_pred_29, reference = cardio_test_y2 )
>>>>>>> main
  cmaccu2 = cm2$overall['Accuracy']
  print( paste("Accuracy_K_29 = ", cmaccu2 ) )
```
                
<<<<<<< HEAD
**After remove `gender` and `bmi`, the accuracy increased from 0.714 to 0.722**, indicating remove these two variables can slightly improve the modeling performance.
=======
**After remove `gender` and `bmi`, the Accuracy increased from 0.717 to 0.722, Precision increased from 0.732 to 0.74, the Specification increased from 0.757 ti 0.766, and the F1 Score increased from 0.703 to 0.753, while the Recall value remained unchanged. These results suggesting remove these two variables can slightly improve the modeling performance, especially improved its ability in predicting the absence of the cardiovascular diseases.**
<<<<<<< HEAD
=======


## **KNN modeling using only `age`, `cholesterol`, `bmi`, `ap_hi` and `ap_lo.**  
## Train / Test split:  
The dataframe was split into training subset (70% of the total dataframe) and test subset (30% of the total dataframe). Only `age`, `cholesterol`, `bmi`, `ap_hi` and `ap_lo` were used for the modeling. 
```{r}
cardio_scale3 <- cardio_scale[c(1, 5:7, 12:13)]
cardio_scale3
set.seed(42)
cardio_sample3 <- sample(2, nrow(cardio_scale3), replace=TRUE, prob=c(0.7, 0.3))
cardio_training_x3 <- cardio_scale3[cardio_sample3==1, 1:5]
cardio_test_x3 <- cardio_scale3[cardio_sample3==2, 1:5]
cardio_training_y3 <- cardio_scale3[cardio_sample3==1, 6]
cardio_test_y3 <- cardio_scale3[cardio_sample3==2, 6]
```

## Looking for the best K value:  
Odd numbers from 3 to 40 were used for the test. The accuracy values from the model that used these K values were shown in the table and the line graph. According the results, K = 31 is the best K value for the KNN modeling.

```{r}
K_test <- seq(3, 40, by = 2)
ResultDf3 = data.frame()
for (k_v in K_test) {
  cardio_pred3 <- knn(train = cardio_training_x3, test = cardio_test_x3, cl=cardio_training_y3, k=k_v)
  cardio_PRED_Cross3 <- CrossTable(cardio_test_y3, cardio_pred3, prop.chisq = FALSE)
  
  cm3 = confusionMatrix(cardio_pred3, reference = cardio_test_y3 )
  
  cmaccu3 = cm3$overall['Accuracy']

  cmt3 = data.frame(k=k_v, Total.Accuracy = cmaccu3, row.names = NULL ) 
  
  ResultDf3 = rbind(ResultDf3, cmt3)
}
```
```{r results='markup'}
xkabledply(ResultDf3, "Total Accuracy Summary")
```

```{r}
library(ggplot2)
ggplot(ResultDf3, aes(x=k, y=Total.Accuracy)) +
  geom_line(color = "skyblue", size = 1.5) +
  geom_point(size = 3, color = "blue") + 
  labs(title = "Total.Accuracy vs K")
```  


Detailed results and the confusion matrix using K=29 were shown below. The following information can be calculated from the confusion matrix:  
* Accuracy = 0.721  
* Precision = 0.737  
* Recall = 0.680  
* Specification = 0.761  
* F1 Score = 0.707  



```{r results='markup'}
  cardio_pred_31 <- knn(train = cardio_training_x3, test = cardio_test_x3, cl=cardio_training_y3, k=31)
  cardio_PRED_Cross3 <- CrossTable(cardio_test_y3, cardio_pred_31, prop.chisq = FALSE)
  cm3 = confusionMatrix(cardio_pred_31, reference = cardio_test_y3 )
  cmaccu3 = cm3$overall['Accuracy']
  print( paste("Accuracy_K_31 = ", cmaccu3 ) )
```
>>>>>>> main
>>>>>>> main
